Questions:
[] Os sparse codes têm sempre um numero de active units multiplo de 3, Porquê?
[] No codigo que gera os sparse codes devo ou nao usar o -mean /std (learn_dictionary function)
[] decoder.py linha 18, retirei uma das dimentions (era o channel???)

Housekeeping:
[] guardar matrizes de bools
[x] mudar nome dos ficheiros
[x] Aprender a fazer SSH para o pc do Tagus
[x] Correr o código com o CUDA no ubuntu.
[x] use wandb
[x] retirar plots das funcoes compute or load, meter essa logica no main
[] usar ConfigArgParse
[x] usar tqdm
[x] migrar os ficheiros para linux subsystem por causa do WSL2

Experiments:
    Code generation:
        [] experiment with/without winner takes all
        [] Ver como é que o Fs influencia a norma no step de convolution da geração de codigos
        [] baseline: guardar versão binaria do MNIST com threshold na willshaw e medir retrive error (como no paper do Luis).
        [x] Medir a "distribution" dos sparse codes usando a KL divergence entre os codes e a uniform distribution.
        [x] Fazer a kl div como deve de ser (normalizada para 1)
        [x] Tentar Shannon Entropy para medir distribuiçao dos codigos
        [] Exprimentar com class-wise feature learning no k-means
    Willshaw:
        [] Usar MLP como pseudo Willshaw para fazer retrieval.
            - Ver quais os valores do Threshold do retrieval process.

    Decoder:
        [x] Tentar o MLP com zero Hidden layers (perceptron)
        [x] Treinar uma deconvolution CNN com as mesmos parametros que geraram os codigos, comparar com aqueles que são obtidos com o KNN do Luis.
        [x] Comparar a accuracy do melhor CCN decoder, com um que seja inicializado com os kernels do Luis.
        [] Fazer plot do MSE de reconstrucao por class (paraver se o modelo esta a dar overfit a uma class especifica)
    Encoder:
    Autoencoder:

[x] Fazer uma funcao que mete as descs a zero, passar as funcoes autoassociation, completion e classification para o main
[x] Exprimentar Pr = 0.0 e Pc= numero baixo para garantir sparseness.

Optimizaçoes:
[] Usar csr_matrix no interval_classifier
[] Usar CSC para column slicing
[] Usar LIL ou DOK para mudanças de sparsity

-----------------------------------------------------------------------------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------------------------------------------------------

Tese:
    Refazer e organizar:
    [] Reproduzir paper do Luis com mais valores na Grid-Search:
        () Fs maior que 2
        () Tw menor que 0.75
        () Exprimentar com wta
        () Exprimentar com background
    [] Experiments da distribution estao uma confusao.
        () Refazer a analise de Q vs KLdiv vs Shannon entropy.
        () Medir apenas a que funciona para a experiemnt de storage.
    [] Dirty Reconstructions.
        () Repetir estas experiments no WANDB.
        () Medir os dois tipos de erro (loss e extra) ao longo do storing.
        () Treinar o MLP e CNN no cod, ret, e ret+cod
    [] Fazer baseline para as descriptions com o binary e X-HOT descriptions.
    [] Verificar se as desCodes pseudo-generations e generations estao sempre a usar a mesma desC (duvida da meeting 10)
    Pontas Soltas:
    [] Pseudo-WN: usar os valores de uma WN treinada para inicializar um sparse hidden layer duma NN.
        () Definir o bias term como o valor médio do threshold de retrieval.
        () Usar uma transfer function parecida a soft thresholding.
    [] Class-wise kmeans feature learning (presentation 6).
        () Fazer PCA do normal vs o CW.
        () Olhar para os valores dos eigen vectors para ver se há um conjunto pequeno de dimensões a ser overfitted.


Paper:
    Refazer e organizar:
        []
    Novas experiments:
        [] Usar a modalidade do tempo, juntamente com os códigos e ww codes (presentation 8).
        []