Questions:
[] Os sparse codes têm sempre um numero de active units multiplo de 3, Porquê?
[] No codigo que gera os sparse codes devo ou nao usar o -mean /std (learn_dictionary function)
[] decoder.py linha 18, retirei uma das dimentions (era o channel???)

Housekeeping:
[] guardar matrizes de bools
[x] mudar nome dos ficheiros
[x] Aprender a fazer SSH para o pc do Tagus
[x] Correr o código com o CUDA no ubuntu.
[x] use wandb
[x] retirar plots das funcoes compute or load, meter essa logica no main
[] usar ConfigArgParse
[x] usar tqdm
[x] migrar os ficheiros para linux subsystem por causa do WSL2

Experiments:
    Code generation:
        [] experiment with/without winner takes all
        [] Ver como é que o Fs influencia a distancia no step de convolution da geração de codigos
        [] baseline: guardar versão binaria do MNIST com threshold na willshaw e medir retrive error (como no paper do Luis).
        [x] Medir a "distribution" dos sparse codes usando a KL divergence entre os codes e a uniform distribution.
        [x] Fazer a kl div como deve de ser (normalizada para 1)
        [x] Tentar Shannon Entropy para medir distribuiçao dos codigos
        [] Exprimentar com class-wise feature learning no k-means
    Willshaw:
        [] Usar MLP como pseudo Willshaw para fazer retrieval.
            - Ver quais os valores do Threshold do retrieval process.

    Decoder:
        [x] Tentar o MLP com zero Hidden layers (perceptron)
        [x] Treinar uma deconvolution CNN com as mesmos parametros que geraram os codigos, comparar com aqueles que são obtidos com o KNN do Luis.
        [x] Comparar a accuracy do melhor CCN decoder, com um que seja inicializado com os kernels do Luis.
        [] Fazer plot do MSE de reconstrucao por class (paraver se o modelo esta a dar overfit a uma class especifica)
    Encoder:
    Autoencoder: